services:
  llm-inference-service:
    image: # Use your own image (that already pushed to docker hub)
    ports:
      - # Host port 8080 maps to container port 5000
    environment:
      # Set the `METRICS_LOG_FILE` value to be `inside_compose_inference_metrics.csv`
    volumes:
      - # Mount volume to get output in you host machine in a file named `compose_inference_metrics.csv`